{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18de80ad-99b8-4753-b79e-d6418c9c8431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix and classification report for selected features:\n",
      "Predicted  B-NEG  I-NEG      O\n",
      "Gold                          \n",
      "B-NEG        172      0      4\n",
      "I-NEG          0      2      1\n",
      "O              5      0  13382\n",
      "P: 0.990459314550456 R: 0.8811886417204612 F1: 0.9247102601712583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-NEG  0.9717514 0.9772727 0.9745042       176\n",
      "       I-NEG  1.0000000 0.6666667 0.8000000         3\n",
      "           O  0.9996265 0.9996265 0.9996265     13388\n",
      "\n",
      "    accuracy                      0.9992629     13567\n",
      "   macro avg  0.9904593 0.8811886 0.9247103     13567\n",
      "weighted avg  0.9992650 0.9992629 0.9992565     13567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Shark\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1.0, 'loss': 'squared_hinge', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "trainfile = 'data/SEM-2012-SharedTask-CD-SCO-training-simple.v2.features.conll'\n",
    "testfile = 'data/SEM-2012-SharedTask-CD-SCO-dev-simple.v2.features.conll'\n",
    "#testfile = 'data/bioscope.clinical.columns.features.conll'\n",
    "training_path_opt ='data/SEM-2012-SharedTask-CD-SCO-training-simple.v2.features.conll'\n",
    "dev_path_opt = 'data/SEM-2012-SharedTask-CD-SCO-dev-simple.v2.features.conll'\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return svm_classifier: a trained SVM classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    svm_classifier = LinearSVC()   \n",
    "    svm_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return svm_classifier, vec\n",
    "\n",
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    ''' \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels[1:], 'Predicted': predictions[1:]    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n",
    "#vectorizer and lr_classifier are the vectorizer and classifiers created in the previous cell.\n",
    "#it is important that the same vectorizer is used for both training and testing: they should use the same mapping from values to dimensions\n",
    "# predictions, goldlabels = get_predicted_and_gold_labels_token_only(testfile, vectorizer, lr_classifier)\n",
    "# print_confusion_matrix(predictions, goldlabels)\n",
    "\n",
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "#feature_to_index = {'TOKEN': 0, 'POS': 1, 'LEMMA': 2, 'PUNCTUATION': 3, 'STARTSWITH_CAPITAL_LETTER': 4, 'IS_STOPWORD': 5}\n",
    "\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    if conllfile.startswith('data/SEM'):\n",
    "        \n",
    "        feature_to_index = {'Token': 3 , 'Pre-token': 5, 'Next-token': 6, 'Lemma':7 , 'Pre-lemma':8 , 'Next-lemma':9, 'POS':10, 'Pre-POS':11 , 'Next-POS':12 , 'POS_classified':13 , 'Punctuation_python': 14, 'MatchesNegExp': 15, 'HasNegAffix':16, 'Negated event':17, 'NegAffix':18}\n",
    "        features = []\n",
    "        labels = []\n",
    "        conllinput = open(conllfile, 'r')\n",
    "        #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "        #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "        #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "        csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "        next(csvreader, None)\n",
    "        for row in csvreader:\n",
    "            #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "            if len(row) > 0:\n",
    "                #structuring feature value pairs as key-value pairs in a dictionary\n",
    "                #the first column in the conll file represents tokens\n",
    "                feature_value = {}\n",
    "                for feature_name in selected_features:\n",
    "                    row_index = feature_to_index.get(feature_name)\n",
    "                    feature_value[feature_name] = row[row_index]\n",
    "                features.append(feature_value)\n",
    "                #The last column provides the gold label (= the correct answer). \n",
    "                labels.append(row[4])\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        feature_to_index = {'Token': 3 , 'Pre-token': 6, 'Next-token': 7, 'Lemma':8 , 'Pre-lemma':9 , 'Next-lemma':10, 'POS':11, 'Pre-POS':12 , 'Next-POS':13 , 'POS_classified':14 , 'Punctuation_python': 15, 'MatchesNegExp': 16, 'HasNegAffix':17, 'Negated event':18, 'NegAffix':19}\n",
    "        features = []\n",
    "        labels = []\n",
    "        conllinput = open(conllfile, 'r')\n",
    "        #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "        #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "        #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "        csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "        next(csvreader, None)\n",
    "        for row in csvreader:\n",
    "            #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "            if len(row) > 0:\n",
    "                #structuring feature value pairs as key-value pairs in a dictionary\n",
    "                #the first column in the conll file represents tokens\n",
    "                feature_value = {}\n",
    "                for feature_name in selected_features:\n",
    "                    row_index = feature_to_index.get(feature_name)\n",
    "                    feature_value[feature_name] = row[row_index]\n",
    "                features.append(feature_value)\n",
    "                #The last column provides the gold label (= the correct answer). \n",
    "                labels.append(row[4])\n",
    "                \n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "def find_best_parameters(trainfile, selected_features):\n",
    "    \"\"\"\n",
    "    Function to find the best parameters for the classifier.\n",
    "    \"\"\"\n",
    "    training_opt = pd.read_csv(training_path_opt, encoding ='utf-8', sep='\\t')\n",
    "    \n",
    "    x_train_opt, _ = extract_features_and_gold_labels(training_path_opt, selected_features)\n",
    "    \n",
    "    # Transforming the features to vectors:\n",
    "    vec = DictVectorizer()\n",
    "    x_train_opt_vec = vec.fit_transform(x_train_opt)\n",
    "    # Adding labels to a seperate list:\n",
    "    y_train_opt = training_opt.Label.to_list()\n",
    "    \n",
    "    classifier = LinearSVC()\n",
    "    parameters = dict(\n",
    "        C = (0.01,0.1,1.0),\n",
    "        loss = ('hinge','squared_hinge'),\n",
    "        tol = (1e-4,1e-3,1e-2,1e-1))\n",
    "\n",
    "    grid = GridSearchCV(estimator = classifier, param_grid=parameters, cv=5, scoring='f1_macro')\n",
    "    grid.fit(x_train_opt_vec, y_train_opt)\n",
    "    classifier = grid.best_estimator_\n",
    "    print('Best parameters:',grid.best_params_)\n",
    "\n",
    "\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "# all_features = ['Token', 'Pre-token', 'Next-token', 'Lemma', 'Pre-lemma', 'Next-lemma', 'POS', 'Pre-POS', 'Next-POS', 'POS_classified', 'Punctuation_python', 'MatchesNegExp', 'HasNegAffix', 'Negated event', 'NegAffix']\n",
    "\n",
    "# print('confusion matrix and classification report for all features:')\n",
    "\n",
    "# sparse_feature_reps, labels = extract_features_and_gold_labels(trainfile, all_features)\n",
    "# #we can use the same function as before for creating the classifier and vectorizer\n",
    "# svm_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)\n",
    "# #when applying our model to new data, we need to use the same features\n",
    "# predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, svm_classifier, all_features)\n",
    "# print_confusion_matrix(predictions, goldlabels)\n",
    "# print_precision_recall_fscore(predictions, goldlabels)\n",
    "# report = classification_report(goldlabels,predictions,digits = 7, zero_division=0)\n",
    "# print(report)\n",
    "\n",
    "\n",
    "print('confusion matrix and classification report for selected features:')\n",
    "\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "selected_features = ['Token', 'Pre-token', 'Next-token', 'Pre-lemma', 'POS', 'Negated event', 'NegAffix', 'HasNegAffix']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "svm_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, svm_classifier, selected_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)\n",
    "report = classification_report(goldlabels,predictions,digits = 7, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "find_best_parameters(trainfile, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e5d331-2ba6-4d23-9bfb-4cc4ea026a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .txt file to easily extract tokens\n",
    "inputfile = 'data/SEM-2012-SharedTask-CD-SCO-dev-simple.v2.txt'\n",
    "df = pd.read_csv(inputfile, sep=\"\\t\", header=None, names=[\"Book\", \"Sent nr\", \"Token nr\", \"Token\", \"Label\"])\n",
    "tokenlist = df['Token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eeb2797-c471-43d6-8855-cc8eccab78dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singular</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experience</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13562</th>\n",
       "      <td>orthodox</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13563</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13564</th>\n",
       "      <td>his</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13565</th>\n",
       "      <td>ritual</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13566</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token true pred\n",
       "0              1.    O    O\n",
       "1             The    O    O\n",
       "2        Singular    O    O\n",
       "3      Experience    O    O\n",
       "4              of    O    O\n",
       "...           ...  ...  ...\n",
       "13562    orthodox    O    O\n",
       "13563          in    O    O\n",
       "13564         his    O    O\n",
       "13565      ritual    O    O\n",
       "13566           .    O    O\n",
       "\n",
       "[13567 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tokens, labels and predictions in dataframe\n",
    "d = {'token': tokenlist, 'true': goldlabels, 'pred': predictions}\n",
    "error_df = pd.DataFrame(d)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69c15cc1-48a4-4cdc-a185-20ce6871baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all matches and errors together in lists\n",
    "matches = [[],[],[]]\n",
    "errors = [[],[],[]]\n",
    "i = 0\n",
    "for token in tokenlist:\n",
    "    if goldlabels[i] == predictions[i]:\n",
    "        matches[0].append(token)\n",
    "        matches[1].append(goldlabels[i])\n",
    "        matches[2].append(predictions[i])\n",
    "    if goldlabels[i] != predictions[i]:\n",
    "        errors[0].append(token)\n",
    "        errors[1].append(goldlabels[i])\n",
    "        errors[2].append(predictions[i]) \n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cec00e-188f-451a-b2ba-a68e3ffbc5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singular</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experience</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13552</th>\n",
       "      <td>orthodox</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13553</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13554</th>\n",
       "      <td>his</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>ritual</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13557 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token true pred\n",
       "0              1.    O    O\n",
       "1             The    O    O\n",
       "2        Singular    O    O\n",
       "3      Experience    O    O\n",
       "4              of    O    O\n",
       "...           ...  ...  ...\n",
       "13552    orthodox    O    O\n",
       "13553          in    O    O\n",
       "13554         his    O    O\n",
       "13555      ritual    O    O\n",
       "13556           .    O    O\n",
       "\n",
       "[13557 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe with all matches\n",
    "d = {'token': matches[0], 'true': matches[1], 'pred': matches[2]}\n",
    "matches_df = pd.DataFrame(d)\n",
    "matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "117958fd-b199-482d-9973-6f7d51a8f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by</td>\n",
       "      <td>B-NEG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nothing</td>\n",
       "      <td>B-NEG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Save</td>\n",
       "      <td>B-NEG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>none</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>never</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>more</td>\n",
       "      <td>I-NEG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>not</td>\n",
       "      <td>B-NEG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token   true   pred\n",
       "0     none      O  B-NEG\n",
       "1       no      O  B-NEG\n",
       "2       by  B-NEG      O\n",
       "3  nothing  B-NEG      O\n",
       "4     none      O  B-NEG\n",
       "5     Save  B-NEG      O\n",
       "6     none      O  B-NEG\n",
       "7    never      O  B-NEG\n",
       "8     more  I-NEG      O\n",
       "9      not  B-NEG      O"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe with all errors\n",
    "d = {'token': errors[0], 'true': errors[1], 'pred': errors[2]}\n",
    "errors_df = pd.DataFrame(d)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec24177f-75bb-45b9-b6ec-7adcd531d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matches statistics\n",
    "prefixes = ['un', 'dis', 'im', 'in', 'non', 'ir']\n",
    "suffixes = ['less', 'lessly', 'lessness']\n",
    "\n",
    "matching_tokens = matches[0]\n",
    "matching_true = matches[1]\n",
    "matching_pred = matches[2]\n",
    "\n",
    "affixes = [[],[]]\n",
    "lexicals = [[],[]]\n",
    "outsides = [[],[]]\n",
    "\n",
    "i = 0\n",
    "for token in matching_tokens:\n",
    "    if token.startswith('un') or token.startswith('dis') or token.startswith('im') or token.startswith('in') or token.startswith('non') or token.startswith('ir') or token.endswith('less') or token.endswith('lessly') or token.endswith('lessness'):\n",
    "        if matching_true[i] != 'O':\n",
    "            affixes[0].append(token)\n",
    "            affixes[1].append(matching_true[i])\n",
    "        else:\n",
    "            outsides[0].append(token)\n",
    "            outsides[1].append(matching_true[i])\n",
    "    else:\n",
    "        if matching_true[i] == 'O':\n",
    "            outsides[0].append(token)\n",
    "            outsides[1].append(matching_true[i])\n",
    "        else:\n",
    "            lexicals[0].append(token)\n",
    "            lexicals[1].append(matching_true[i])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b27a058a-dab8-4e0a-8733-373182565597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches O total: 13383\n",
      "Matches ALL NEGS affixal:33\n",
      "Matches ALL NEGS lexical: 141\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "print(f\"Matches O total: {len(outsides[0])}\")\n",
    "print(f\"Matches ALL NEGS affixal: {len(affixes[0])}\")\n",
    "print(f\"Matches ALL NEGS lexical: {len(lexicals[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dccb58f7-2704-4550-b624-9b5868d17b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches B-NEG affixal: 33\n",
      "Matches I-NEG affixal: 0\n"
     ]
    }
   ],
   "source": [
    "# more stats (affixal)\n",
    "b_neg = []\n",
    "i_neg = []\n",
    "\n",
    "i = 0\n",
    "for item in affixes[0]:\n",
    "    if affixes[1][i] == 'B-NEG':\n",
    "        b_neg.append((item, affixes[1][i]))\n",
    "    elif affixes[1][i] == 'I-NEG':\n",
    "        i_neg.append((item, affixes[1][i]))\n",
    "    i +=1\n",
    "\n",
    "print(f\"Matches B-NEG affixal: {len(b_neg)}\")\n",
    "print(f\"Matches I-NEG affixal: {len(i_neg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93b09628-ecbf-4c93-94c9-e452fefc2cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches B-NEG lexical: 139\n",
      "Matches I-NEG lexical: 2\n"
     ]
    }
   ],
   "source": [
    "# even more stats (lexical)\n",
    "b_neg = []\n",
    "i_neg = []\n",
    "\n",
    "i = 0\n",
    "for item in lexicals[0]:\n",
    "    if lexicals[1][i] == 'B-NEG':\n",
    "        b_neg.append((item, lexicals[1][i]))\n",
    "    elif lexicals[1][i] == 'I-NEG':\n",
    "        i_neg.append((item, lexicals[1][i]))\n",
    "    i +=1\n",
    "\n",
    "print(f\"Matches B-NEG lexical: {len(b_neg)}\")\n",
    "print(f\"Matches I-NEG lexical: {len(i_neg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354d995-22f7-442f-b482-9289f4fbd60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
